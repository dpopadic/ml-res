{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mfml.resources.data import load_mnist\n",
    "MNIST = load_mnist()\n",
    "images = MNIST['data'].astype(np.double)\n",
    "labels = MNIST['target'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images[0].reshape(28,28), cmap='gray');\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $M$ principal components. \n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Data normalization (`normalize`).\n",
    "2. Find eigenvalues and corresponding eigenvectors for the covariance matrix $S$.\n",
    "   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n",
    "3. Compute the orthogonal projection matrix and use that to project the data onto the subspace spanned by the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization `normalize`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalize the given dataset X to have zero mean & 1 unit of standard deviation.\n",
    "    Args:\n",
    "        X: ndarray, dataset of shape (N,D)\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean): tuple of ndarray, Xbar is the normalized dataset\n",
    "        with mean 0 & standard deviation of 1.\n",
    "    \"\"\"\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0)\n",
    "    # it can occur that there is NAN's when computing stdev, impute 1's\n",
    "    sd_ed = sd.copy()\n",
    "    sd_ed[sd == 0] = 1.\n",
    "    x_norm = (X - mu) / sd_ed\n",
    "    return x_norm, mu, sd_ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute eigenvalues and eigenvectors `eig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and corresponding eigenvectors\n",
    "        for the covariance matrix S.\n",
    "    Args:\n",
    "        S: ndarray, covariance matrix\n",
    "\n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n",
    "\n",
    "    Note:\n",
    "        the eigenvals and eigenvecs should be sorted in descending\n",
    "        order of the eigen values\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eig(S)\n",
    "    sort_ind = np.argsort(eigvals)[::-1]\n",
    "      \n",
    "    return eigvals[sort_ind], eigvecs[:, sort_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next given a orthonormal basis spanned by the eigenvectors, we will compute the projection matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_matrix(B):\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
    "    Args:\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\n",
    "    \n",
    "    Returns:\n",
    "        P: the projection matrix\n",
    "    \"\"\"\n",
    "    P = (B@(np.linalg.inv(B.T@B))@B.T)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, num_components, recon_origin=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "        recon_origin: whether to return the reconstruction matrix normalised (True) or not\n",
    "    Returns:\n",
    "        the reconstructed data, the sample mean of the X, principal values\n",
    "        and principal components\n",
    "    \"\"\"\n",
    "    X_norm, mean, sd = normalize(X)\n",
    "    S = np.cov(X_norm, rowvar=False, bias=True)\n",
    "    # find eigenvalues and corresponding eigenvectors for S\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "    # take the top `num_components` of eig_vals and eig_vecs,\n",
    "    # this will be the corresponding principal values and components\n",
    "    principal_vals, principal_components = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "    P = projection_matrix(principal_components)\n",
    "    if recon_origin:\n",
    "        reconst = (P @ X_norm.T).T * sd + mean\n",
    "    else:\n",
    "        reconst = (P @ X_norm.T).T\n",
    "    # reconstruct the data from the using the basis spanned by the principal components\n",
    "    # Notice that we have subtracted the mean from X and divided by the stdev, so make sure \n",
    "    # that you add it back to the reconstructed data\n",
    "    return reconst, mean, principal_vals, principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None, label=None):\n",
    "    \"\"\"Draw a vector from v0 to v1.\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, \n",
    "                    color='k')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops, label=label)\n",
    "\n",
    "# visualize what PCA does on a 2D toy dataset.\n",
    "mvn = scipy.stats.multivariate_normal(\n",
    "    mean=np.ones(2), \n",
    "    cov=np.array([[1, 0.8], [0.8, 1]])\n",
    ")\n",
    "\n",
    "X = mvn.rvs((100,), random_state=np.random.RandomState(0))\n",
    "\n",
    "num_components = 1\n",
    "X_reconst, mean, principal_values, principal_components = PCA(X, num_components)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], label='data')\n",
    "for (princial_variance, principal_component) in (zip(principal_values, principal_components.T)):\n",
    "    draw_vector(\n",
    "        mean, mean + np.sqrt(princial_variance) * principal_component, \n",
    "        ax=ax)\n",
    "ax.scatter(X_reconst[:, 0], X_reconst[:, 1], label='reconstructed')\n",
    "plt.axis('equal');\n",
    "plt.legend();\n",
    "ax.set(xlabel='$\\mathbf{x}_0$', ylabel='$\\mathbf{x}_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255.\n",
    "Xbar, mu, std = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How many principal components do we need\n",
    "> in order to reach a Mean Squared Error (MSE) of less than 100 for our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "reconstructions = []\n",
    "for num_component in range(1, 100, 5):\n",
    "    reconst, _, _, _ = PCA(Xbar, num_component, recon_origin=False)\n",
    "    error = mse(reconst, Xbar)\n",
    "    reconstructions.append(reconst)\n",
    "    print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.asarray(reconstructions)\n",
    "reconstructions = reconstructions * std + mu # bring back to original space of the reconstructed image\n",
    "loss = np.asarray(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:,0], loss[:,1]);\n",
    "ax.axhline(100, linestyle='--', color='r', linewidth=2)\n",
    "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers don't tell us much. Just what does it mean qualitatively for the loss to decrease from around 450 to less than 100? In the next cell, we draw the original eight as the leftmost image. Then we show the reconstruction of the image on the right, in descending number of principal components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, 1000))\n",
    "def show_num_components_reconst(image_idx):\n",
    "    fig, ax = plt.subplots(figsize=(20., 20.))\n",
    "    actual = X[image_idx]\n",
    "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
    "    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]),\n",
    "              cmap='gray');\n",
    "    ax.axvline(28, color='orange', linewidth=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
