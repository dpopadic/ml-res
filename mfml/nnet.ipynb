{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "## Instructions\n",
    "\n",
    "We will train a neural network to draw a curve.\n",
    "The curve takes one input variable, the amount travelled along the curve from 0 to 1, and returns 2 outputs, the 2D coordinates of the position of points on the curve.\n",
    "\n",
    "To help capture the complexity of the curve, we shall use two hidden layers in our network with 6 and 7 neurons respectively.\n",
    "\n",
    "![nn](img/nn_example_graph.png)\n",
    "\n",
    "We will calculate the Jacobian of the cost function, with respect to the weights and biases of the network. It will form part of a stochastic steepest descent algorithm that will train the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "In the following cell, we will define functions to set up our neural network.\n",
    "Namely an activation function, $\\sigma(z)$, it's derivative, $\\sigma'(z)$, a function to initialise weights and biases, and a function that calculates each activation of the network using feed-forward.\n",
    "\n",
    "Recall the feed-forward equations,\n",
    "$$ \\mathbf{a}^{(n)} = \\sigma(\\mathbf{z}^{(n)}) $$\n",
    "$$ \\mathbf{z}^{(n)} = \\mathbf{W}^{(n)}\\mathbf{a}^{(n-1)} + \\mathbf{b}^{(n)} $$\n",
    "\n",
    "In this worksheet we will use the *logistic function* as our activation function, rather than the more familiar $\\tanh$.\n",
    "$$ \\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp(-\\mathbf{z})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "# initialise the network with it's structure\n",
    "def reset_network (n1 = 6, n2 = 7, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(2, n2) / 2\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(2, 1) / 2\n",
    "\n",
    "# feed forward each activation to the next layer and return all weighted sums and activations.\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "    return a0, z1, a1, z2, a2, z3, a3\n",
    "\n",
    "# Cost function of a neural network with respect to a training set.\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Starting with layer 3, which is the easiest, and work backwards through the layers we derive the Jacobian of the cost function with respect to the weights and biases.\n",
    "\n",
    "\n",
    "\n",
    "Jacobians:,\n",
    "$$ \\mathbf{J}_{\\mathbf{W}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} $$\n",
    "$$ \\mathbf{J}_{\\mathbf{b}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} $$\n",
    "etc., where $C$ is the average cost function over the training set. i.e.,\n",
    "$$ C = \\frac{1}{N}\\sum_k C_k $$\n",
    "for the weight:\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}}\n",
    "   ,$$\n",
    "for the bias:\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}}\n",
    "   .$$\n",
    "partial derivatives:\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}} = 2(\\mathbf{a}^{(3)} - \\mathbf{y}) $$\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}} = \\sigma'({z}^{(3)})$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}} = \\mathbf{a}^{(2)}$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}} = 1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the third layer weights:\n",
    "def J_W3 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    J = 2 * (a3 - y)\n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = J * d_sigma(z3)\n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = J @ a2.T / x.size\n",
    "    return J\n",
    "\n",
    "# Jacobian for the bias.\n",
    "def J_b3 (x, y) :\n",
    "    # set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # the first two partial derivatives of the Jacobian.\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    # we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # Need to sum over all training examples however.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll nNext do the Jacobian for the Layer 2. The partial derivatives for this are,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{W}^{(2)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}\n",
    "   .$$\n",
    "This is very similar to the previous layer, with two exceptions:\n",
    "* There is a new partial derivative, in parentheses, $\\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}$\n",
    "* The terms after the parentheses are now one layer lower.\n",
    "\n",
    "Recall the new partial derivative takes the following form,\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\sigma'(\\mathbf{z}^{(3)})\n",
    "   \\mathbf{W}^{(3)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_W2 (x, y) :\n",
    "    # the first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    J = 2 * (a3 - y)\n",
    "    # the next two lines implement da3/da2, first Ïƒ' and then W3.\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    return J\n",
    "\n",
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 1 is very similar to Layer 2, but with an addition partial derivative term.\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}\n",
    "   .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = J @ a0.T / x.size\n",
    "    return J\n",
    "\n",
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
